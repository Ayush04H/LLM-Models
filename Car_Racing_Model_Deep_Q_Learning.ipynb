{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6y65IDwGSWA8txfVVFoBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush04H/LLM-Models/blob/main/Car_Racing_Model_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Gymnasium**"
      ],
      "metadata": {
        "id": "HIHYFatZ6JFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nn-jodjNuYey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4abd97-dd26-4a7f-bcf5-e64d22feb1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.2)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=001edb73e2b50f38074dd587573f89a1dd383ade390a471a25130ccbcefc2f47\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,402 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349144 sha256=1675cbf31fe2088001f6e52dc8a555f00e75e0d1be998f566e060bfe63260d79\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the Basic Libraries Needed**"
      ],
      "metadata": {
        "id": "6YmswlaCYLv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ],
      "metadata": {
        "id": "ZSD-zT5b0c6S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the AI**"
      ],
      "metadata": {
        "id": "HCJFvbI2YW4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating The Arcihtecture of the Neural Network"
      ],
      "metadata": {
        "id": "CePCWhReN7NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "subpifONNdgg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('CarRacing-v2')\n",
        "\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_space_type = type(env.action_space)\n",
        "\n",
        "if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "    number_actions = env.action_space.n\n",
        "else:\n",
        "    number_actions = env.action_space.shape[0]\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ],
      "metadata": {
        "id": "j10ZKc6q0fIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e99c3-e813-4cb5-bae3-bfba60efbd23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (96, 96, 3)\n",
            "State size:  96\n",
            "Number of actions:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training The AI**"
      ],
      "metadata": {
        "id": "6nDytKbycKiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting The Environment"
      ],
      "metadata": {
        "id": "C2WcCF1fcOBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 100\n",
        "discount_factor = 0.99\n",
        "replay_buffer_size = int(1e5)\n",
        "interpolation_parameter = 1e-3"
      ],
      "metadata": {
        "id": "fV_Z5dU70hoI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing The HyperParameters"
      ],
      "metadata": {
        "id": "AJ4ttASldkBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k = batch_size)\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "    return states, next_states, actions, rewards, dones"
      ],
      "metadata": {
        "id": "nBotkjr60iUO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Experience Replay"
      ],
      "metadata": {
        "id": "J1AcDHfJh7fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > minibatch_size:\n",
        "        experiences = self.memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ],
      "metadata": {
        "id": "STDx_xFD0kS4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement DQN Class"
      ],
      "metadata": {
        "id": "HX7m7w4Th_EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, number_actions)"
      ],
      "metadata": {
        "id": "UwB2_Uvm0nax"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializng DQN Agent"
      ],
      "metadata": {
        "id": "3k6OxzUGiFcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen=100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(maximum_number_timesteps_per_episode):\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "    scores_on_100_episodes.append(score)\n",
        "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end=\"\")\n",
        "    if episode % 100 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "    if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100,\n",
        "                                                                                        np.mean(scores_on_100_episodes)))\n",
        "        torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "        break\n"
      ],
      "metadata": {
        "id": "TWSaiuzk0rz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "606471ce-63fe-4a0f-a1f9-a807a1745e95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got tuple)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-39efa272404b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_number_timesteps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-5fc70fabdbf0>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_qnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training The DQN Agent"
      ],
      "metadata": {
        "id": "s3GFVj5H8jcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1JwF_oVzCBt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v2')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "_-LRMI_Y0toW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "y5VaR-aKB46e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "k8k0MGfR8rEM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFEdpbmt0wy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}